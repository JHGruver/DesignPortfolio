# Case Study Validation & Interview Prep Guide
### Perpetuals Group — CPO Interview Preparation

*Review every claim in both case studies. For each one: can you explain it, source it, and stand by it under scrutiny?*

---

## How to Use This Document

For each claim, ask yourself:
1. **Can I explain the "how" behind this number?** (Not just the what)
2. **Would I bet my credibility on this?** (If not, soften the language or have a caveat ready)
3. **What's the follow-up question a CPO would ask?** (Have the answer loaded)

---

---

# CASE STUDY 1: HyperPrompts — AI Aggregator SaaS Platform

## Role & Scope Claims

| Claim | As Written | Prep Notes |
|-------|-----------|------------|
| **Sole Designer** | "Sole Designer — End-to-End UX/UI" | Be ready to explain: Did you have any contractors, freelancers, or co-founders doing design work? "Sole designer" means zero other design contributors. If a founder sketched wireframes or a developer made UI decisions, mention that context. |
| **Duration** | 7 Months (Jul 2023 – Feb 2024) | Confirm these dates match your resume and LinkedIn. CPOs cross-reference. |
| **Tools** | Figma, Notion, Miro | Be ready to explain how each was used: Figma for design/prototyping, Notion for documentation/specs, Miro for workshops/affinity mapping. |

### Likely CPO Question:
> "You were the only designer — who gave you feedback? How did you avoid designing in a vacuum?"

**Your answer should cover:** Founder feedback loops, user testing sessions, how you used research data to validate rather than relying on internal opinions.

---

## Metrics & Numbers

### $250K Seed Funding

| Detail | What's Written | What You Need to Know |
|--------|---------------|----------------------|
| Claim | "The team's $250K fundraise" | Good — this is framed as a team outcome. |
| Your role | "Pitch deck and prototypes were central to the raise" | Be specific: Did you design the pitch deck? The interactive prototype they demoed? Both? Which slides were yours? |
| Context | "Pitch deck and interactive prototypes helped the team secure $250K in seed funding" | |

**Prep question:** *"Walk me through the investor meeting. What did they see that you designed?"*
- How many investors did you pitch?
- Was it one round or multiple meetings?
- What specific design artifacts were in the room?
- Did the investors give feedback on the design specifically?

### $140K Client Sales

| Detail | What's Written | What You Need to Know |
|--------|---------------|----------------------|
| Claim | "$140K Client Sales Generated" | |
| Your role | "White-label design system enabled $140K in early client contracts" | Be clear: the design system made it possible to customize quickly, which helped close deals. You didn't personally sell. |
| Follow-up | How many clients? What was the contract structure? | You should know: Was this 1 big client or several? Were these LOIs or signed contracts? |

**Prep question:** *"How did the design system specifically enable those sales?"*
- What could you customize in the white-label system?
- How fast could you spin up a branded instance?
- What did the clients actually see in their demos?

### 8 AI Tools at Launch (was 30+)

| Detail | What's Written | What You Need to Know |
|--------|---------------|----------------------|
| Claim | "MVP launched with 8 integrated AI tools — the architecture was designed to scale to 30+" | This is now honest. Know which 8 tools shipped. |

**Prep question:** *"Which 8 tools shipped? How did you prioritize?"*
- Name them. If you can't name all 8, that's a red flag.
- What was the selection criteria? (User demand? API availability? Cost?)
- What was on the roadmap but didn't make the cut?

### A/B Test Results

| Detail | What's Written | What You Need to Know |
|--------|---------------|----------------------|
| Sample size | 10 participants (5 per variant) | Honest and disclosed. |
| Results | "~2 clicks vs ~3 clicks", "~9/10 vs ~7/10 satisfaction" | Directional, not falsely precise. Good. |
| Framing | "Small sample, clear signal" | |
| Caveat | "Directional, not statistically significant — but consistent enough to commit to sidebar." | |

**Prep question:** *"With only 10 participants, how confident were you in this decision?"*
- The signal was unanimous — every sidebar participant found tools faster
- You supplemented with qualitative feedback (what did participants say?)
- The cost of being wrong was low (could always change nav later)
- You were making a startup decision, not publishing a paper

### 12+ User Interviews

| Detail | What's Written | What You Need to Know |
|--------|---------------|----------------------|
| Claim | "12+ interviews with AI researchers and knowledge workers" | Know the real number. Was it 12? 15? 20? |
| Method | Interviews (not surveys) | In-person? Video call? How long were they? |
| Output | "Identified 3 core persona types" | Be ready to explain how you synthesized 12 interviews into 3 personas. What was the affinity mapping process? |

**Prep question:** *"Tell me about your most surprising interview finding."*
- Have 2-3 specific moments ready. Not generic insights — actual "I didn't expect that" moments.

---

## Competitive Analysis Table

| Tool | Price Listed | Verify |
|------|-------------|--------|
| ChatGPT | $20/mo | This was the ChatGPT Plus price in mid-2023. Correct. |
| Claude | $20/mo | Claude Pro was $20/mo at that time. Correct. |
| Midjourney | $10/mo | Basic plan was $10/mo in 2023. Correct. |

**Prep question:** *"How did you conduct the competitive analysis? 30+ tools is a lot."*
- Did you use all 30+ tools yourself, or did you do desk research for some?
- What was the evaluation framework?
- How long did this take?

---

## Design Principles

| Principle | As Written | Why It Works |
|-----------|-----------|-------------|
| One Prompt, Many Models | "Users shouldn't need to re-type when switching between AI tools." | Specific to this product. Good. |
| Free Gets You In, Value Keeps You | "Lower the barrier with a real free tier..." | Shows business thinking. 50 prompts/month decision is specific. |
| Show, Don't Compare | "Side-by-side outputs speak louder than feature checklists." | Demonstrates product insight. |
| Team-First Architecture | "Enterprise needs drove the structure even at MVP stage." | Shows forward thinking. Be ready to explain what this meant concretely. |

**Prep question:** *"You said enterprise needs drove MVP architecture. What did that look like in practice?"*
- Shared workspaces from day 1? Billing system? Permissions model?
- What did you actually build vs. plan for later?

---

## Strategic Questions Section

| Question | Answer Given | Verify |
|----------|-------------|--------|
| Build or aggregate? | "We chose aggregate — faster to market" | Why was aggregate faster? What would building have looked like? |
| How much free? | "50 prompts/month" | Is this the real number? Can you explain the math behind it? |
| One workspace or per-tool? | "Single workspace won in testing" | Which test? Was this part of the A/B or a separate test? |

---

## Behind the Scenes Stories

These are the highest-scrutiny items. A CPO will probe these for authenticity.

### Story 1: Three Founders, Three Visions (Week 2)

| Element | What's Written | Can You Explain? |
|---------|---------------|-----------------|
| Setup | "One wanted a marketplace, one wanted a prompt library, one wanted enterprise-only" | Know which founder wanted what. Use first names if comfortable. |
| Method | "Printed out every interview quote on sticky notes... 2-hour workshop" | Where did this happen? (Office? WeWork? Someone's apartment?) How many sticky notes roughly? |
| Turning point | "Two of the three visions had almost no supporting quotes, the room went quiet" | Which vision won? (It should align with what you built) |

**If asked to elaborate:** Describe the physical space, the wall of sticky notes, the moment of silence. Sensory details = credibility.

### Story 2: The Participant Who Quit (Week 4)

| Element | What's Written | Can You Explain? |
|---------|---------------|-----------------|
| What happened | "She said 'I don't know where anything is' and closed the laptop" | Was this remote or in-person? What tool were you using for the test? |
| Your reaction | "I felt my stomach drop" | Authentic detail. Keep this. |
| Key insight | "Every other participant hesitated at the exact same point" | At what point? What were the original labels? |
| Change made | "Rewrote from tool-centric ('Claude,' 'GPT-4') to task-centric ('Write,' 'Analyze,' 'Generate')" | Can you show the before/after in your Figma file? |

**If asked to elaborate:** What did the other 4 participants say about the same area? How did you identify the pattern across sessions?

### Story 3: 48 Hours to Investor Demo (Week 6)

| Element | What's Written | Can You Explain? |
|---------|---------------|-----------------|
| Problem | "Three broken interactions and placeholder screenshots" | Which interactions were broken? |
| Decision | "Polished exactly 3 screens and 12 slides" | Which 3 screens? Why those? |
| Outcome | "Investors never saw the rough edges, and we closed the round" | This is the $250K raise? Or a different meeting? |

### Story 4: The Figma File Incident (Week 3)

| Element | What's Written | Can You Explain? |
|---------|---------------|-----------------|
| Problem | "Shared library update had broken half my component overrides" | What library was it? (A team library? A public one?) |
| Triage | "Fixed the 8 screens in the presentation flow" | What was the stakeholder review about? |
| Solution | "Started versioning shared libraries separately" | How? Branch-based? Separate Figma file? |

---

## Persona Quotes

| Quote | Attribution | Verify |
|-------|-----------|--------|
| "I spend more time managing subscriptions..." | Product Manager, Series B Startup | Anonymized. Good. Can you recall the actual interview context? |
| "There's no easy way to compare outputs..." | AI Researcher, University Lab | Same — can you recall this conversation? |
| "I need something that works across my entire team..." | Creative Director, Digital Agency | Same. |

**These are now anonymized correctly.** If asked "are these real quotes?", say yes, anonymized for privacy. Be ready to paraphrase the broader conversation around each quote.

---

---

# CASE STUDY 2: CloudCar — In-Vehicle Infotainment for JLR

## Role & Scope Claims

| Claim | As Written | Prep Notes |
|-------|-----------|------------|
| **Role** | "UX/UI Designer — Feature Refinement" | Good framing. Not claiming lead or senior. |
| **Scope** | "I joined CloudCar mid-development; the core architecture and visual language were already established by a prior team. My job was to improve specific flows — media source switching, account setup, and profile management — without breaking what already worked." | This is your armor. Memorize this framing. It shows maturity and honesty. |
| **Duration** | 22 Months (Jan 2017 – Nov 2018) | Verify dates match resume. This is a long engagement — be ready to explain the timeline. Were you full-time the whole 22 months? |
| **Tools** | Adobe XD, Photoshop, Illustrator | Not Figma. This was 2017. Makes sense chronologically. If asked why not Figma: "Figma wasn't widely adopted in automotive/enterprise contexts in 2017." |

### Likely CPO Question:
> "You said you joined mid-development. What was the state of the product when you arrived?"

**Your answer should cover:**
- What generation was in production (Gen 2)
- What specific problems existed (58% error rate, confusing nav, long setup)
- What the prior team had already built
- How you assessed what to keep vs. change

---

## Metrics & Numbers

### 30M+ JLR Fleet (Installed Base)

| Detail | What's Written | What You Need to Know |
|--------|---------------|----------------------|
| Claim | "JLR ships millions of vehicles annually. The media app I refined is part of that fleet's standard infotainment stack." | Reframed as JLR's number, not yours. Good. |
| Label | "JLR Fleet (Installed Base)" | |

**Prep question:** *"How do you know the media app was deployed across the whole fleet?"*
- Was it standard on all JLR models or specific trims?
- Which model years included your refinements?
- Was it Gen 3 only, or did changes go back to Gen 2?

### 30% Error Reduction (58% → 28%)

| Detail | What's Written | What You Need to Know |
|--------|---------------|----------------------|
| Claim | "30 percentage point reduction in usability errors (58% → 28%)" | Note: 30 percentage POINTS, not 30%. The actual percentage reduction is ~52%. The case study correctly says "30 percentage point reduction." |
| Lab vs Production gap | "The jump from 15% (lab) to 28% (production) reflects real-world conditions — slower hardware, varied screen sizes, and edge-case media sources not in our test matrix." | This is your prepared answer for the gap. Memorize it. |
| Method | "In-vehicle testing conducted with 12 participants across US and UK markets" | |

**Prep question:** *"Walk me through how you measured error rate."*
- What counted as an "error"? (Failed task? Wrong tap? Timeout?)
- How many tasks per session?
- Were the same tasks used for Gen 2 baseline and refined version?
- Who ran the testing? (You? A research team? CloudCar QA?)

**Prep question:** *"Why did the final production error rate (28%) jump from the Cycle 3 lab result (15%)?"*
- You have the answer ready: real-world conditions, hardware variation, edge cases
- Can you name a specific edge case? (e.g., "a media source that had a different API response time than our test sources")

### Competitor Error Rates

| Competitor | Rate | What You Need to Know |
|-----------|------|----------------------|
| BMW iDrive | 45% | |
| Audi MMI | 52% | |
| Tesla | 38% | |
| Sourcing | "Benchmarked internally by CloudCar's QA team using identical task sets across competitors" | Were you present for these benchmarks? Or did QA hand you the data? |

**Prep question:** *"How did you benchmark competitors — did you have access to their vehicles?"*
- Was it CloudCar's QA team testing in actual competitor vehicles?
- Same task set across all vehicles?
- When were these benchmarks done? (Before your refinement or during?)

### 22-Month Duration

**Prep question:** *"22 months is a long time. Walk me through the phases."*
- Jan-Jun 2017: Onboarding, research, initial refinement
- Jul-Dec 2017: Cycles 2 and 3, media hierarchy, account polish
- 2018: Final release cycle, production handoff
- Were you full-time on this the whole time, or split with other projects?

### Account Setup Time (5m → 2m)

| Detail | What's Written | What You Need to Know |
|--------|---------------|----------------------|
| Claim | "60% faster account setup" (5 min → 2 min) | How was this measured? Timed during testing sessions? |

---

## NHTSA Safety Reference

| Detail | What's Written | What You Need to Know |
|--------|---------------|----------------------|
| Claim | "Every interaction was evaluated against NHTSA visual-manual guidelines: no task should require more than 2 seconds of eyes-off-road or 12 seconds total glance time." | The 2-second / 12-second rule is real NHTSA guidance. Verify you actually tested against this. |

**Prep question:** *"How did you measure eyes-off-road time?"*
- Did you use eye-tracking? Timer-based estimation? NHTSA-standard methodology?
- Was this done in a lab with a simulator, or in real vehicles?
- Did CloudCar have formal NHTSA compliance requirements, or was this a self-imposed standard?

**The 2s/12s numbers are from:** NHTSA's "Visual-Manual NHTSA Driver Distraction Guidelines for In-Vehicle Electronic Devices" (2013). These are real guidelines used across the automotive industry.

---

## Design Decisions

### Persistent Now Playing Bar

| Detail | What's Written | What You Need to Know |
|--------|---------------|----------------------|
| Impact | "roughly halved the time drivers spent navigating media controls" | Previously said "40%." Now softened. Good. Can you explain how you measured this? Before/after timed tasks? |
| Why it won | "Drivers always knew their current context" | What was the alternative? (Contextual menus? Swipe-up gesture?) |

### 3-Tap Rule

| Detail | What's Written | What You Need to Know |
|--------|---------------|----------------------|
| Claim | "any media task had to complete in 3 taps or fewer, validated against NHTSA glance-time guidelines" | Did ALL media tasks actually meet this? Any exceptions? |

**Prep question:** *"What media task was hardest to get down to 3 taps?"*
- This is where you show depth. Have a specific flow ready (e.g., "Switching from Bluetooth to a specific Spotify playlist — that was originally 5 taps, and we had to rethink the source hierarchy to get it to 3.")

---

## User Quotes

| Quote | Attribution | Verify |
|-------|-----------|--------|
| "I just want to play my music without taking my eyes off the road." | Research Participant, Daily Commuter | Anonymized. Can you recall the context? |
| "Switching sources takes too many steps." | Daily Commuter, Range Rover Owner | Same. |
| "Profiles should remember preferences." | Fleet Manager, Corporate Accounts | Same. Was this a fleet manager at a specific company? |
| "Clarity is safety." | Product Director, JLR Connected Car | This is an internal stakeholder, not a user. Fine to keep. |
| "The refinements turned this from 'good enough' to flagship-level design." | Senior Stakeholder, JLR Connected Car | Can you say who this was (by title, not name) if pressed? VP? Director? |

---

## Reflection Claims

### Cross-Continent Collaboration
| Claim | "Working with development teams across the US and UK" |
|-------|------------------------------------------------------|
| Prep | Which teams were where? What was the handoff process? What time zone challenges did you face? |

### Driver Shadowing Regret
| Claim | "I would have introduced real driver shadowing earlier" |
|-------|-------------------------------------------------------|
| Prep | Did you do ANY driver shadowing? Or is this purely hypothetical? If hypothetical, be clear: "We tested in vehicles, but I wish I'd gone further and observed real commutes." |

---

---

# CROSS-CASE STUDY: Patterns a CPO Will Notice

## Template Symmetry
Both case studies now have different subtitles and tonal emphasis:
- **HyperPrompts** leans startup/business (funding, MVP, investor demos)
- **CloudCar** leans craft/safety (NHTSA, driver safety, error reduction)

This is intentional and good. If asked why they feel different: "Different contexts demand different storytelling. A startup story is about speed and validation. An automotive story is about rigor and safety."

## Your Growth Arc
A CPO will look for growth between the two projects:
- **CloudCar (2017-2018):** You were part of a larger team, refining existing work, learning automotive constraints
- **HyperPrompts (2023-2024):** You were the sole designer, end-to-end, with business outcomes tied to your work

**Be ready to articulate:** "CloudCar taught me how to work within constraints I didn't set. HyperPrompts taught me how to set the constraints myself."

## Honesty Signals the CPO Will Appreciate
Things in your case studies that signal authenticity:
- "8 at launch, architecture designed to scale to 30+" (not overclaiming)
- "Small sample, clear signal" (acknowledging limitations)
- "Pitch deck and prototypes were central to the raise" (not "I secured $250K")
- "I joined mid-development" (not claiming ownership of the whole product)
- The error rate gap explanation (showing you understand lab vs. production)
- "What I'd Do Differently" sections (self-awareness)

## Red Flags to Avoid in Conversation
- Don't say "I secured the funding" — say "my design work was central to the fundraise"
- Don't say "I shipped to 30M vehicles" — say "the flows I refined are part of JLR's standard infotainment stack"
- Don't cite specific percentages from the A/B test as if they're statistically significant
- Don't describe the competitive analysis as if you personally used all 30+ tools extensively

---

---

# QUICK-REFERENCE: Numbers at a Glance

## HyperPrompts
| Number | What It Is | Source |
|--------|-----------|--------|
| $250K | Seed funding (team outcome) | Fundraise records |
| $140K | Client sales (team outcome) | Contract records |
| 7 months | Jul 2023 – Feb 2024 | Project timeline |
| 8 | AI tools at MVP launch | Product records |
| 30+ | Tools in roadmap/architecture | Product roadmap |
| 10 | Usability test participants | Test sessions |
| 12+ | User interviews conducted | Interview records |
| 3 | Design iteration cycles | Project timeline |
| ~9/10 | Final user satisfaction | Post-test survey |
| 50 | Free tier prompts/month | Pricing decision |
| $20/mo | ChatGPT/Claude price (2023) | Public pricing |
| $10/mo | Midjourney price (2023) | Public pricing |

## CloudCar
| Number | What It Is | Source |
|--------|-----------|--------|
| 30M+ | JLR installed fleet base | JLR public data |
| 30% | Error rate reduction (pp) | 58% → 28% |
| 22 months | Jan 2017 – Nov 2018 | Project timeline |
| 58% | Gen 2 error rate (baseline) | Internal QA testing |
| 28% | Final production error rate | Production metrics |
| 15% | Cycle 3 lab error rate | Lab testing |
| 12 | In-vehicle test participants | Test sessions |
| 4 | Refinement cycles | Project timeline |
| 5m → 2m | Account setup time reduction | Timed testing |
| 3 | Max taps for any media task | Design rule |
| 2s / 12s | NHTSA glance-time limits | NHTSA guidelines (2013) |
| 45% / 52% / 38% | BMW/Audi/Tesla error rates | CloudCar QA benchmarks |

---

---

# INTERVIEW DAY CHECKLIST

Before the interview, confirm you can:

- [ ] Name the 8 AI tools that shipped in HyperPrompts MVP
- [ ] Describe the 3 founders and what each wanted
- [ ] Walk through the investor pitch meeting (what was on screen, who was in the room)
- [ ] Explain what "white-label" meant concretely (what could be customized)
- [ ] Recall at least 2 specific user interview moments from HyperPrompts
- [ ] Describe the CloudCar product state when you joined (what existed, what didn't)
- [ ] Name the specific flows you worked on at CloudCar (media switching, account setup, profile management)
- [ ] Explain the error rate gap (15% lab → 28% production) without looking at notes
- [ ] Describe one specific in-vehicle testing moment (glare, button size, specific flow that broke)
- [ ] Articulate NHTSA 2s/12s rule and how you tested against it
- [ ] Explain the 3-tap rule and give an example of a flow that was hard to simplify
- [ ] Describe the cross-continent collaboration logistics (tools, meetings, handoff process)
- [ ] Articulate your growth arc from CloudCar (2017) to HyperPrompts (2023)
